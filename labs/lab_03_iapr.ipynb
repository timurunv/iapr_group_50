{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022c879b-e00c-468f-854f-aa9c798796b9",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 ‒  Classification\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 02.04.2025   \n",
    "**Due date:** 16.04.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`  \n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.  \n",
    "- **You are not allowed to use any libraries** other than those provided in this notebook.  \n",
    "- **TAs must be able to run your code from start to finish without any issues.**  \n",
    "- **Failure to follow these guidelines may result in point deductions** during grading.  \n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e88e80-cbae-4f05-ada3-cc46c0efbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install pillow -q\n",
    "!pip install pandas -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install tqdm -q\n",
    "!pip install gdown -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is at least python 3.9\n",
    "import sys \n",
    "assert (sys.version_info.major == 3) and (sys.version_info.minor == 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da76e6-851c-4cc7-852b-fcedac869495",
   "metadata": {},
   "source": [
    "Please take note that PyTorch will be utilized in this lab. PyTorch is a widely recognized library for deep learning. Prior to commencing the lab, we kindly ask you to review this quick tutorial available [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df3391-feb0-47b0-b9f9-50eea0e802d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "# Get os name\n",
    "os_name = platform.system().lower()\n",
    "\n",
    "# OS X\n",
    "if os_name == 'darwin':\n",
    "    print(\"Detected OS X\")\n",
    "    %pip install torch torchvision torchaudio -q\n",
    "    \n",
    "#Windows\n",
    "elif os_name == 'windows':\n",
    "    print(\"Detected Windows\")\n",
    "    %pip install torch torchvision torchaudio -q\n",
    "    \n",
    "# Linux  \n",
    "else:\n",
    "    print(\"Detected Linux\")\n",
    "    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main packages\n",
    "import os\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Callable\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "from utils.lab_03_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30adfc3",
   "metadata": {},
   "source": [
    "# Real-World Image Classification in Histopathology [30 points]\n",
    "\n",
    "Supervised learning for classifying histopathology images, despite its capabilities, faces significant challenges. A primary obstacle is its reliance on labeled data, which is often scarce and costly to acquire due to the need for expert annotations. This scarcity can impede model performance, particularly when dealing with uncommon diseases or subtle pathological patterns. Moreover, supervised models may struggle with generalization to unseen data or variations in tissue staining protocols. \n",
    "\n",
    "This lab will concentrate on constructing classification models tailored for histopathology using the least labeled data possible. Since the lab does not center on computing descriptors for the images, only image features will be provided. Specifically, we utilized [CTransPath](https://github.com/Xiyue-Wang/TransPath), one of the most robust and precise existing image feature extractors in histopathology, as the descriptor. Your focus will be on classifying these features for downstream tasks crucial to histopathological analysis.\n",
    "\n",
    "Before starting, download the data by running the following cell and make sure the data are located as follows:\n",
    "```code\n",
    "├── labs\n",
    "│   └──lab_03_iapr.ipynb\n",
    "└── data\n",
    "    └── data_lab_03\n",
    "        ├── part_01\n",
    "        │    ├── k16_train.pth\n",
    "        │    ├── k16_val.pth\n",
    "        │    ├── k16_test.pth\n",
    "        │    ├── k16.png\n",
    "        │    └── k16.svg\n",
    "        └── part_02\n",
    "            ├── acinar.png\n",
    "            ├── solid.png\n",
    "            ├── wsi.png\n",
    "            ├── DHMC_0001.png\n",
    "            ├── DHMC_0007.png\n",
    "            ├── dhmc_train.pth\n",
    "            ├── dhmc_val.pth\n",
    "            └── dhmc_test.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29c863",
   "metadata": {},
   "source": [
    "## Part 1 - Tumor, Stroma Classification in Colorectal Cancer Histopathology (11 points)\n",
    "\n",
    "Colorectal cancer ranks among the most prevalent cancers affecting both men and women. Accurate diagnosis, supplemented with prognostic and predictive biomarker information, plays a pivotal role in patient monitoring and facilitating personalized treatment approaches. One crucial biomarker is the Tumor/Stroma ratio (TSR) observed in unhealthy colon tissues. This ratio serves as an indicator of cancer invasiveness, with higher ratios correlating to increased invasiveness and, consequently, diminished patient survival probabilities.\n",
    "\n",
    "Traditionally, pathologists assess the TSR by visually inspecting unhealthy tissue samples under a microscope, relying on their expertise to estimate the ratio. However, given the large volume of samples and the occasional lack of precision in estimations, there arises a pressing need for automated recognition of various tissue types within histological images. The development of a multi-class classifier becomes imperative to accurately identify the diverse tissue types present. Typically, these tissue types include TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE, and EMPTY (background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_figure(\"../data/data_lab_03/part_01/k16.png\",\"Fig1: Collection of tissue types in colorectal cancer histology.\",(16,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbd0ad",
   "metadata": {},
   "source": [
    "Until now, state-of-the-art methodologies in histology have predominantly relied on deep-learning-based supervised learning techniques. However, a significant drawback of such an approach lies in the requirement for access to a meticulously annotated training dataset. Annotating histological data poses considerable challenges—it is a time-consuming process that demands the expertise of pathologists. Moreover, annotators are compelled to label every tissue type, even though only two (TUMOR and STROMA) are of primary interest.\n",
    "\n",
    "To address these challenges, we propose an alternative approach. To streamline the annotation process, we task the annotator with labeling only the tissues of interest (TUMOR and STROMA), and discarding the rest. Subsequently, we aim to train a binary classifier capable of automatically recognizing these specific tissues during testing. This novel approach seeks to alleviate the burden of annotation while still enabling accurate identification of the critical tissue types. In this section, we will proceed with implementing and evaluating this proposed methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data(\"k16_train.pth\")\n",
    "val_x, val_y = load_data(\"k16_val.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cf19f",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance [2.5 pts]\n",
    "\n",
    "Your task is to construct this binary classifier utilizing the Mahalanobis distance as taught in class. Begin by executing the cell below to load the training and validation features for TUMOR and STROMA. These features have been computed using a self-supervised model tailored for histopathology, known as CTransPath. Note that label `0` corresponds to TUMOR and label `1` to STROMA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec65d58",
   "metadata": {},
   "source": [
    "* **Q1 (1 pt)**: Complete the `fit` method in `MahalanobisClassifier`. This method calculates the parameters necessary for the Mahalanobis Classifier when fitted to the training data.\n",
    "* **Q2 (1 pt)**: Complete the `predict` method in `MahalanobisClassifier`. This method is responsible for predicting the class for each test feature as well as the distance to class means using the Mahalanobis distance method.\n",
    "\n",
    "**Note**: It is forbidden to use any prebuilt Mahalanobis distance function. You may only use `LedoitWolf` in `sklearn.covariance` for computing a stable covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6976bb7-f856-4980-9268-fc66be80dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisClassifier:\n",
    "    \"\"\"Mahalanobis based classifer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            means (torch.tensor): (n_classes, d) Mean of the features for each class\n",
    "            inv_covs (torch.tensor): (n_classes, d, d) Inverse of covariance matrix across d features for each class   \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.means = None\n",
    "        self.inv_covs = None\n",
    "        \n",
    "    def fit(self, train_x : torch.Tensor, train_y : torch.Tensor):\n",
    "        \"\"\"Computes parameters for Mahalanobis Classifier (self.mean and self.cov), fitted on the training data.\n",
    "\n",
    "        Args:\n",
    "            train_x (torch.Tensor): (N, d) The tensor of training features\n",
    "            train_y (torch.Tensor): (N,) The tensor of training labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Define number of classes\n",
    "        n_classes = len(np.unique(np.unique(train_y)))\n",
    "        n, d = train_x.shape\n",
    "        \n",
    "        # Set default values\n",
    "        means = torch.zeros((n_classes, d), dtype=train_x.dtype)\n",
    "        inv_covs = torch.ones((n_classes, d, d), dtype=train_x.dtype)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "            \n",
    "        self.means = means\n",
    "        self.inv_covs = inv_covs\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts the class of every test feature, using the Mahalanobis Distance\n",
    "\n",
    "        Args:\n",
    "            test_x (torch.Tensor): (N, d) The tensor of test features\n",
    "\n",
    "        Returns:\n",
    "            preds (torch.Tensor): (N,) The predictions tensor (id of the predicted class {0, 1, ..., n_classes-1})\n",
    "            dists (torch.Tensor): (N, n_classes) Mahalanobis distance from sample to class means\n",
    "        \"\"\"\n",
    "\n",
    "        # Define default output value\n",
    "        N, d = test_x.shape\n",
    "        dists = torch.zeros((N, self.means.shape[0]), dtype=test_x.dtype)\n",
    "        preds = torch.zeros(N, dtype=test_x.dtype)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        return preds, dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5f505",
   "metadata": {},
   "source": [
    "* **Q3 (0.5 pt)**: After fitting your classifier on the training data, compute the accuracy of the validation data. Are you satisfied with the results?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96687c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "mahalanobis_classifier(\n",
    "    MahalanobisClassifier, train_x, train_y, val_x, val_y,\n",
    "    cls_name=[\"Tumor\", \"Stroma\"], colors=[\"r\", \"b\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96c344",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance [3.5 pts]\n",
    "\n",
    "Your classifier appears to perform well. However, during testing, it's possible for other tissue types to be present, which cannot be manually filtered out. Moreover, these tissue types may not be recognized by the model as they fall outside the labeled training distribution (It is the consequence of the laziness of the annotators ;)). Therefore, it's crucial to filter out these out-of-distribution (OoD) samples.\n",
    "\n",
    "One approach to OoD detection involves computing an OoD-ness score for each test example. This score should be low for in-distribution (ID) examples and high for OoDs. Subsequently, a threshold is defined, for which any example with a greater OoD-ness is discarded, while those below it are forwarded to the model for prediction. An example of an OoD-ness score is the minimum Mahalanobis distance to means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0140d67",
   "metadata": {},
   "source": [
    "* **Q1 (0.5 pts)**: Why do you think the minimum Mahalanobis distance is a good OoD-ness score?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87ad75-97a9-4937-8f92-beaf0f8edda1",
   "metadata": {},
   "source": [
    "Start by running the cell below to load the test set. It comprises TUMOR and STROMA samples, along with other tissue types. Note that OoD tissues types are labeled to `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = load_data(\"k16_test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffad9ca",
   "metadata": {},
   "source": [
    "* **Q2 (0.5 pts)**: We create a new classifier `MahalanobisOODClassifier` that inherits from the previous one. Reimplement the function `predict` such that it returns as well the OoD scores. We define the `ood_scores` as the minimum Mahalanobis distance from the classifier. Your accuracy results should be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20bd6-ca9e-4656-bd97-49fc1a1506a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisOODClassifier(MahalanobisClassifier):\n",
    "    \"\"\"Predicts the class of every test feature, using the Mahalanobis Distance\n",
    "\n",
    "    Args:\n",
    "        test_x (torch.Tensor): (N x d) The tensor of test features\n",
    "\n",
    "    Returns:\n",
    "        preds (torch.Tensor): (N,) The predictions tensor (id of the predicted class {0, 1, ..., n_classes-1})\n",
    "        dists (torch.Tensor): (N, n_classes) Mahalanobis distance from sample to class means\n",
    "        ood_scores (torch.Tensor): (N,) Score of OoDness as the minimal distance from the sample to classes\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get super prediction (from MahalanobisClassifier)\n",
    "        preds, dists = super().predict(test_x=test_x)\n",
    "        N = preds.shape[0]\n",
    "\n",
    "        # Assign dummy values to scores\n",
    "        ood_scores = np.zeros(N)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return preds, dists, ood_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72c271-a0d2-4e25-bcb3-31d8019a18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "classifier_ood, val_y_ood_scores=mahalanobis_ood_classifier(\n",
    "    MahalanobisOODClassifier, train_x, train_y, val_x, val_y,\n",
    "    cls_name=[\"Tumor\", \"Stroma\"], colors=[\"r\", \"b\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d56315-272b-4c5e-ba61-8a2e0f6c25c4",
   "metadata": {},
   "source": [
    "* **Q3 (0.5 pts)**: Based on the validation set OoD scores, determine a threshold for the minimum Mahalanobis distance such that 95% of the validation samples are identified as ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb80fc-90f5-49d5-ae8d-b579a237ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ood_threshold(ood_scores, quantile=0.95):\n",
    "    \"\"\" Get OoD threshold based on measured scores and quantile\n",
    "\n",
    "    Args:\n",
    "        ood_scores (torch.Tensor): (N, ) N measured OoDness scores\n",
    "        quantile (float): Percentage of samples that are considered as in distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default value\n",
    "    threshold = 0\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb771f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "threshold_val = check_threshold(get_ood_threshold,val_y_ood_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b92ae",
   "metadata": {},
   "source": [
    "* **Q4 (2 pts)**: Complete the function `compute_metrics` that computes the recall for TUMOR, STROMA, and OoD examples as well as the average recall over the 3 classes. To do so, you need to consider OoDs as a third class by assigning the prediction `-1` to filtered-out examples based on your threshold. Based on your results, conclude on the feasibility of the proposed pipeline. Propose a solution that would require the least annotation possible but that could significantly increase your OoD recall.\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdfb02-a3f5-4f72-86b4-4399db6cf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y, y_hat, ood_scores, threshold):\n",
    "    \"\"\" Compute recall for tumor, stroma, and OoD as well as the average recall.\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): (N) Class ground truth {-1, 0, 1, ..., n_classes}\n",
    "        y_hat (torch.Tensor): (N,) Class predictions {0, 1, ..., n_classes}\n",
    "        ood_scores (torch.Tensor): (N, ) N measured OoDness scores\n",
    "        threshold (float): OoD threshold\n",
    "    \"\"\"\n",
    "    # Define variable with dummy values \n",
    "    recall_tumor = 0\n",
    "    recall_stroma = 0\n",
    "    recall_ood = 0\n",
    "    avg_recall = 0\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return recall_tumor, recall_stroma, recall_ood, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "# Predictions on test set\n",
    "test_y_dist, test_y_hat = eval_test(classifier_ood,compute_metrics, test_x,test_y,threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2442b2d",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-Distribution detection with k-NN [5 pts]\n",
    "\n",
    "Let's explore another technique based on k-Nearest Neighbors (k-NN). The features utilized have been extracted from a self-supervised model, known for their efficacy as k-NN classifiers. This motivates us to implement a k-NN classifier for identifying TUMOR and STROMA. Additionally, the k-NN distance serves as a suitable OoD-ness score, aligning well with our task requirements.\n",
    "\n",
    "* **Q1 (2 pts)**: Complete the `fit` and `predict` functions in `kNNClassifier`. Also, assign your own handcrafted OoD score in `predict`. We recommend to use `top_k` function from torch ([doc](https://pytorch.org/docs/stable/generated/torch.topk.html)). When using k>1, use majority voting to select the winning class.\n",
    "\n",
    "**Note**: It is forbidden to use any prebuilt k-NN classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d39391-e446-47ca-836c-c49d8f0ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNClassifier:\n",
    "    \"\"\"k-NN based classifier\"\"\"\n",
    "\n",
    "    def __init__(self, k : int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k (int): The number of neighbors to consider for the classification\n",
    "            features (torch.Tensor): (N, d) feature of the N train samples\n",
    "            labels (torch.Tensor): (N,) labels for train samples\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, train_x : torch.Tensor, train_y : torch.Tensor):\n",
    "        \"\"\"Store training data parameters (features and labels) for k-NN classifier.\n",
    "\n",
    "        Args:\n",
    "            train_x (torch.Tensor): (N, d) The tensor of training features\n",
    "            train_y (torch.Tensor): (N,) The tensor of training labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get size and default values\n",
    "        N, d = train_x.shape\n",
    "        features = torch.zeros((N, d))\n",
    "        labels = torch.zeros(N)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts the class of every test feature, using the k-NN\n",
    "\n",
    "        Args:\n",
    "            test_x (torch.Tensor): (N x d) The tensor of test features\n",
    "\n",
    "        Returns:\n",
    "            preds (torch.Tensor): (N,) The tensor of class predictions {0, 1, ..., n_classes}\n",
    "            ood_scores (torch.Tensor): (N,) The OoD score predictions\n",
    "        \"\"\"\n",
    "\n",
    "                \n",
    "        # Get size and default values\n",
    "        N, d = test_x.shape\n",
    "        preds = torch.zeros(N)\n",
    "        ood_scores = torch.zeros(N)\n",
    "\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return preds, ood_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40473834",
   "metadata": {},
   "source": [
    "* **Q2 (1 pt)**: Find the best `k` among `[1, 3, 5, 9, 15, 25]` based on the validation set. What is the best `k` and accuracy?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5c1f3-95b6-4e11-adee-fe8b95faaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best k for knn fitting (to find among suggested ks)\n",
    "\n",
    "def find_best_k(ks,kNNClassifier: Callable,train_x: torch.Tensor, train_y: torch.Tensor, val_x: torch.Tensor, val_y: torch.Tensor):\n",
    "    best_k = 0\n",
    "    best_accuracy = 0.\n",
    "    # Iterate over ks\n",
    "    for k in ks:\n",
    "\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        continue\n",
    "\n",
    "    return best_k, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "best_k, best_accuracy = check_best_k(find_best_k, kNNClassifier,train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e07e16-0983-4139-8e57-6fd2e8c33b33",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Compute the threshold such that 95% of validation samples are detected as ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b5ca6-48c4-4d9b-a801-9174ce7e5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn(best_k, train_x, train_y, val_x, val_y):\n",
    "\n",
    "    # best threshold\n",
    "    threshold_val = 0\n",
    "    # Predicted val ood scores\n",
    "    val_y_ood_scores = torch.zeros(len(val_y))\n",
    "    classifier = None \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return classifier, threshold_val, val_y_ood_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c61cc2-dee3-4f71-8416-96a2decaeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "# Plot ood scores and threshold\n",
    "classifier, threshold_val, val_y_ood_scores = fit_knn(best_k, train_x, train_y, val_x, val_y)\n",
    "plot_ood_scores(ood_scores=val_y_ood_scores, threshold=threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f9d26-6d43-4bf9-b69f-64e679f3d527",
   "metadata": {},
   "source": [
    "* **Q4 (1 pt)**: We evaluate your classifier on the test set like in `1.2 Q4`. Is it better than Mahalanobis distance? Why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcc55-b63d-41b4-bd02-d120d0d8d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "# Predictions on test set\n",
    "eval_test_knn(classifier, compute_metrics, test_x,test_y,threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d345",
   "metadata": {},
   "source": [
    "## Part 2 - Lung Adenocarcinoma Classification (19 points)\n",
    "\n",
    "In the previous exercise, we successfully detected TUMOR and STROMA tissues using a minimum of labels. This allows us to compute the Tumor-Stroma Ratio (TSR), a valuable indicator for determining tumor grade and guiding treatment decisions. However, despite saving annotations, the need for hundreds of tumor/stroma annotations remains prohibitively expensive. Additionally, associating the TSR value with the correct tumor grade and treatment necessitates further algorithmic developments.\n",
    "\n",
    "An alternative approach involves annotating entire tumor grades on Whole Slide Images (WSIs) and training a classifier directly. However, a significant challenge arises due to the immense size of WSIs, typically containing millions of pixels, which makes direct preprocessing infeasible for computers. To address this challenge, we partition the WSI into thousands of non-overlapping patches. Consequently, each WSI comprises thousands of patch features. However, classifying a conglomerate of features is inherently challenging, especially considering that each WSI may not necessarily contain the same number of patches.\n",
    "\n",
    "As illustrated in Fig. 2, a highly effective method is to apply an aggregation pooling function, which transforms the pool of features into a single slide feature. Subsequently, we can train a simple classifier to classify the slides. In this section, your objective is to develop a classifier and various pooling methods for classifying WSIs of lung adenocarcinoma patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d735434",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_figure(\"../data/data_lab_03/part_02/wsi.png\",\"Fig2: WSI classification pipeline.\",(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bf60f",
   "metadata": {},
   "source": [
    "### 2.1 Dataset [1 pt]\n",
    "\n",
    "Your objective is to classify lung adenocarcinoma patterns, specifically acinar and solid patterns. Acinar adenocarcinoma typically exhibits glandular structures resembling small sacs when viewed under a microscope, as depicted in Fig. 3. These structures may appear irregular and crowded. Conversely, solid adenocarcinoma, as illustrated in Fig. 4, appears as solid sheets or nests of cells with little to no glandular differentiation.\n",
    "\n",
    "You'll be working with a subset of the DHMC dataset, comprising 53 acinar examples and 48 solid examples. To assist you, we provide a training set and a validation set, representing 60% and 40% of the data, respectively. In this dataset, features of the patches composing each Whole Slide Image (WSI) have already been extracted using CTransPath. Your initial task is to prepare the data to be suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_2_figures(\"../data/data_lab_03/part_02/acinar.png\",\"Fig3: Example acinar lung adenocarcinoma WSI.\",\"../data/data_lab_03/part_02/solid.png\",\"Fig4: Example solid lung adenocarcinoma WSI.\",(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ef915-793e-4496-9119-911b34e1c29e",
   "metadata": {},
   "source": [
    "* **Q1 (1 pt)**: Please complete the `DHMC2Cls` class. The class loads raw data from an external file and stores them in `raw_data`. You are required to implement the `__len__` and `__getitem__` functions. The `__len__` function should return the length M of the dataset, while the `__getitem__` function should return a tuple containing: (1) the patch features, (2) the WSI label, (3) the WSI id, and (4) the patch coordinates. Before proceeding take time to investigate the content of the raw data. It's important to note that in the training mode, you should only return the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b020b-8df8-48de-a430-2e3affbfe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHMC2Cls(Dataset):\n",
    "    \"\"\"DHMC dataset using 2 classes\"\"\"\n",
    "\n",
    "    def __init__(self, features_path : str, train : bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            raw_data (list of dict): (M) List of M slides raw data as dictionaries. \n",
    "            train (bool): True if data are the training set. False otherwise\n",
    "            \n",
    "        Args:\n",
    "            features_path (str): The path to the features file\n",
    "            train (bool): Whether it is the training dataset or not\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        # Load raw data from path\n",
    "        self.raw_data = torch.load(features_path, weights_only=False)\n",
    "        # Set if training or not\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: The length M of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        n_data = 0\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return n_data\n",
    "    \n",
    "    def __getitem__(self, index : int):\n",
    "        \"\"\"Returns the entry at index from the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): the requested entry index of the dataset\n",
    "\n",
    "        Returns:\n",
    "            features (torch.Tensor): (N, d) Feature tensor of the selected slide with N patches and d feature dimensions\n",
    "            label (int): Ground truth label {0, ..., n_classes}\n",
    "            wsi_id (str): Name of the WSI as \"DHMC_xxx\" where xxx is a unique id of the slide (train == False only)\n",
    "            coordinates (torch.Tensor): (N, 2) xy coordinates of the N patches of the selected slide (train == False only)\n",
    "        \"\"\"\n",
    "\n",
    "        features = None\n",
    "        label = None\n",
    "        wsi_id = None\n",
    "        coordinates = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        if self.train:\n",
    "            return features, label\n",
    "        else:\n",
    "            return features, label, wsi_id, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522af831-1900-4d51-9b3e-8c2970c133e0",
   "metadata": {},
   "source": [
    "In the cell below, we create the dataset using DHMC2Cls and we test your training and validation datasets to check for inconsistencies. Your implementation should pass all tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f1eb2-3bec-464f-b47f-ac95a55a9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "train_loader, val_loader = create_dataset(DHMC2Cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcbe32",
   "metadata": {},
   "source": [
    "### 2.2 Average Pooling [1 pt]\n",
    "\n",
    "You will start with the simplest pooling method, i.e. average pooling. It simply consists of averaging the WSI patch features to form a single one representative of the WSI.\n",
    "\n",
    "* **Q1 (1 pt)**: Complete the `forward` function in `AveragePooling`. Remember, it takes a set of WSI features with shape `(N x d)`, and should return a single WSI feature of shape `(1 x d)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30d28c-d569-4727-8d05-56a311e5172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePooling(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features : torch.Tensor):\n",
    "        \"\"\" Perform mean along the first dimension of the tensor\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): (N, D) Feature to perform average pooling on\n",
    "        Return:\n",
    "            mean (torch.Tensor): (1, D) Features average over all patches\n",
    "        \"\"\"\n",
    "\n",
    "        mean = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac6879-a1f7-41e6-b3ed-f5aee3a3e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "sanity_check_avg(AveragePooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da1078",
   "metadata": {},
   "source": [
    "### 2.3 Classifier [8 pts]\n",
    "\n",
    "Now that you have coded your first aggregation method, let's build the linear classifier.\n",
    "\n",
    "* **Q1 (3 pts)**: \n",
    "    * Complete the `Classifier` class below. You should fill `__init__` which assigns the attributes. Attributes are :\n",
    "        * `proj` is a nonlinear projection layer that adapts the features for the task. It is simply a linear layer that projects features of dimension `d` to features of lower dimension `H`. Then it is followed by a ReLU.\n",
    "        * `pool`, the pooling function.\n",
    "        * `fc`, the final linear classifier layer. \n",
    "    * Complete `forward` which given a pool of features of shape `(1 x N x d)` outputs the class prediction logits of shape `(1 x 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137088df-a6b6-4066-9d15-abf7a1e95d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim : int, H : int, n_classes : int, pooling_fn : nn.Module) -> None:\n",
    "        \"\"\"Constructs the linear classifier\n",
    "\n",
    "        Attributes:\n",
    "            proj (Callable): Projection of layer (N, d) -> (N, H)\n",
    "            pool (Callable): Pooling layer (N, H) -> (1, H)\n",
    "            fc (Callable): Classification layer (1, H) -> (1, n_classes)\n",
    "            \n",
    "        Args:\n",
    "            in_dim (int): The dimension of input features\n",
    "            H (int): Target dimension for the projection layer\n",
    "            n_classes (int): The number of classes for the task\n",
    "            pooling_fn (nn.Module): The pooling function to aggregate the features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        proj_layer = None\n",
    "        pool_layer = None\n",
    "        fc_layer = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        self.proj = proj_layer\n",
    "        self.pool = pool_layer\n",
    "        self.fc = fc_layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward path\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (1, N, d) Input feature for a given slide with N patches\n",
    "        Return:\n",
    "            logits (torch.Tensor): (1, n_classes) Output logits for classification\n",
    "        \"\"\"\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e19f40-3230-46b9-9819-dd248ecf7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "sanity_check_cls(LinearClassifier,AveragePooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c89ff",
   "metadata": {},
   "source": [
    "The classifier is ready to train. It remains to write the code to optimize your model.\n",
    "\n",
    "* **Q2 (2 pts)**: Please complete the `train` function below. This function should take `train_loader`, `val_loader`, `n_epochs`, and an `optimizer` as inputs. It is responsible for training the `model` and returning the best model checkpoint, best F1 score, and the epoch at which the best F1 score was achieved on the validation set.\n",
    "\n",
    "**Notes**: \n",
    "* Refer to this [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for guidance on training a classifier.\n",
    "* To obtain the model checkpoint, simply call `model.state_dict()`.\n",
    "* We provide you the `test` function, in utils file, that computes the F1 score on a given test dataset. **You should not modify it !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757eac4-d786-4b0f-a7cf-7aaf04b88293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : nn.Module, train_loader : DataLoader, val_loader : DataLoader, n_epochs : int, optimizer : torch.optim.Optimizer):\n",
    "    \"\"\"Trains the neural network self.model for n_epochs using a given optimizer on the training dataset.\n",
    "    Outputs the best model in terms of F1 score on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train\n",
    "        train_loader (DataLoader): The training dataloader to iterate on the training dataset\n",
    "        val_loader (DataLoader): The validation dataloader to iterate on the validation dataset\n",
    "        n_epochs (int): The number of epochs, i.e. the number of time the model should see each training example\n",
    "        optimizer (torch.optim.Optimizer): The optimizer function to update the model parameters\n",
    "\n",
    "    Returns:\n",
    "        best_model (nn.Module): Best model state dictionary \n",
    "        best_f1 (float): Best F1-score on the validation set\n",
    "        best_epoch (int): Best epoch on validation set\n",
    "        val_f1s (list of floats): (n_epochs, ) F1-scores for all epochs\n",
    "        val_losses (list of floats): (n_epochs, ) Losses for all validation epochs\n",
    "        train_losses(list of floats): (n_epochs, ) Losses for all training epochs\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variable to return\n",
    "    best_model = model.state_dict()\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return best_model, best_f1, best_epoch, val_f1s, val_losses, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6225d-e7ff-4680-a92d-8b85ec244e35",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Train a linear classifier using `AveragePooling` for `30` epochs, employing the `Adam` optimizer with a learning rate of `1e-3`. No need to search for optimal hyperparameters. Refer to the PyTorch documentation for guidance on constructing your optimizer. Use `H=512`. Don't worry, the training might take ~10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34334c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d, H, M, n_classes = None, None, None, None \n",
    "epochs = None\n",
    "model = None\n",
    "optimizer = None \n",
    "\n",
    "# ------------------\n",
    "# Your code here ... \n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d638e2-8c0a-4787-9dc8-550dc68a4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "# Train and Plot results\n",
    "plot_training(model, train, train_loader, val_loader, epochs=epochs, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8d802-6d5a-4082-8bc0-6d33fdda432c",
   "metadata": {},
   "source": [
    "* **Q4 (1 pt)**: Are you satisfied with the results? What is the primary disadvantage of employing average pooling? Does the model overfit the data? (justify)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9919f",
   "metadata": {},
   "source": [
    "### 2.4 Attention Pooling [9 pts]\n",
    "\n",
    "Now you will build a more advanced pooling method, called attention pooling. The motivation for this method should result from your analysis in `2.3 Q4`. So we will not share much information with you on this one. Instead, we refer you to the related paper [here](https://arxiv.org/pdf/1802.04712.pdf).\n",
    "\n",
    "* **Q1 (4 pts)**: Complete `Attn_Net_Gated` which implements the gated attention mechanism described in the paper. Note `L`, and `M` are the dimension of the projection weights. You will find similar notations in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9c7be-77d0-4dbe-9e16-fd4850004299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Net_Gated(nn.Module):\n",
    "    def __init__(self, L : int, M : int):\n",
    "        \"\"\"\n",
    "        Attention Network with Sigmoid Gating (3 fc layers)\n",
    "        Args:\n",
    "            L: input feature dimension\n",
    "            M: hidden layer dimension\n",
    "        \"\"\"\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward path of the gated attention network\n",
    "\n",
    "        Args:\n",
    "            xin: (N, L) List of N patches and L features\n",
    "        Return:\n",
    "            A: (N, 1) Attention value for each patch\n",
    "        \"\"\"\n",
    "        A = torch.zeros((1,), dtype=x.dtype)\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35f4d-4371-4dc3-9b24-a3def7176571",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "sanity_gated(Attn_Net_Gated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5140c",
   "metadata": {},
   "source": [
    "* **Q2 (3 pts)**: Complete `AttentionPooling`, which performs attention pooling with the help of the gated attention mechanism. In `forward`, you should only return the attention if `attention_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991c782-1385-45c3-8256-f8e678bb30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, L : int, M : int):\n",
    "        super().__init__()\n",
    "        # Intatiate the gated layer\n",
    "        self.attention_net = Attn_Net_Gated(L, M)\n",
    "\n",
    "    def forward(self, x, attention_only : bool = False):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): (N, L) Input feature over N patches and L features\n",
    "            attention_only (bool): Say whether to return the attention or not\n",
    "        Returns:\n",
    "            Y (torch.Tensor): (1, N) Output, if attention_only==False\n",
    "            A (torch.Tensor): (1, M) Attention values, if attention_only==True\n",
    "        \"\"\"\n",
    "\n",
    "        A = None\n",
    "        Y = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        # Check if need to return attention\n",
    "        if attention_only:\n",
    "            return A\n",
    "        else:\n",
    "            return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0784cd",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Train your linear classifier using `AttentionPooling` with `M=256`. You will train your model for `30` epochs, employing the `Adam` optimizer with a learning rate of `1e-4`. No need to search for optimal hyperparameters. Use `H=512`. Don't worry, the training should take ~5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf008582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d, H, M, n_classes = None, None, None, None \n",
    "epochs = None \n",
    "model = None\n",
    "optimizer = None \n",
    "\n",
    "# ------------------\n",
    "# Your code here ... \n",
    "# ------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a58df-1907-4c2b-9d3c-96e51e31f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "# Train and Plot results\n",
    "plot_training(model, train, train_loader, val_loader, epochs=epochs, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cb8e7",
   "metadata": {},
   "source": [
    "We have kept one example per class for testing and visualization. We have `DHMC_0001.jpg` presenting solid adenocarcinoma patterns and `DHMC_0007.jpg` with acinar adenocarcinoma. Those examples have never been seen in training and validation.\n",
    "\n",
    "* **Q4 (1 pt)**: Test your best attention model on the test dataset below. Use `load_state_dict()` to load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = load_data_2(DHMC2Cls, \"dhmc_test.pth\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Create a model from the best model state\n",
    " \n",
    "model = None\n",
    "\n",
    "# ------------------\n",
    "# Your code here ... \n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151bbe2",
   "metadata": {},
   "source": [
    "As stated in the paper, a benefit of incorporating an attention layer is the enhanced interpretability of the model's decision-making process. This feature is particularly crucial for ensuring the safe deployment of deep learning models, especially in sensitive domains such as the medical field. With an attention layer, it becomes possible to discern the most critical patches that the model considered for a particular decision. To visualize this, please execute the cell below to observe the attention maps on the test examples. **Don't forget to answer the question in the end !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16851826",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "plot_attention(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfe680",
   "metadata": {},
   "source": [
    "* **Q5 (1 pt)**: From the visualization above, what can you interpret?\n",
    "    * **Answer**: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
