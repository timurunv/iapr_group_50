{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4BXjF6YIa1t"
   },
   "source": [
    "# [IAPR][iapr]: Lab 2 â€’  Object description\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 26.03.2025  \n",
    "**Due date:** 09.04.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`  \n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.  \n",
    "- **You are not allowed to use any libraries** other than those provided in this notebook.  \n",
    "- **TAs must be able to run your code from start to finish without any issues.**  \n",
    "- **Failure to follow these guidelines may result in point deductions** during grading.  \n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "############ Check it is python 3.9 ##################\n",
    "## DO NOT COMMENT, WE WILL TEST YOUR CODE WITH 3.9 ###\n",
    "######################################################\n",
    "\n",
    "import sys \n",
    "assert (sys.version_info.major == 3) and (sys.version_info.minor == 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wget -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install scikit-image -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install python-mnist -q\n",
    "!pip install opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main packages\n",
    "from utils.lab_02_utils import *\n",
    "from skimage.morphology import remove_small_objects, remove_small_holes, closing, disk, opening\n",
    "from skimage.transform import rotate, resize\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "In this lab, we will work with the famous MNIST dataset. It is composed of thousands of images (size 28x28) that depict handwritten digits from 0 to 9. The code below will automatically download the data from the online repo. This lab aims to create discriminant features from handwritten digits using various approaches. \n",
    "\n",
    "\n",
    "Take a look at the data to get a better idea of what you will be working within this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_lab02_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1 - Preprocessing [2 pts]\n",
    "\n",
    "\n",
    "In this lab, we will create different feautre descriptors from digits. However, we will not use all images in the dataset. We will focus on the digits \"0\" and \"4\".\n",
    "\n",
    "## 1.1 Selection (1 pt)\n",
    "\n",
    "**Q1 (1 pt)** Your first task is to complete the function `extract_label` such that it selects from the input data only the images that are labeled as a given `target_label`. This function will be used to extract 0s and 4s from the data cohort. When running the code, the plots should only show you samples that are 0s (first plot) and 4s (second plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(images: np.ndarray, labels: np.ndarray, target_label: int):\n",
    "    \"\"\"\n",
    "    The function returns only the images that have target_label as labels.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images - handwritten digits \n",
    "    labels: np.ndarray (N)\n",
    "        List of labels associated with the input image\n",
    "    target_label: int\n",
    "        Selected target label\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    img_extract: np.ndarray (M, 28, 28)\n",
    "        Extracted images that have target_label as label (M should be lower than N).\n",
    "    \"\"\"\n",
    "\n",
    "    n, d, _ = np.shape(images) \n",
    "    img_extract = np.zeros((30, d, d))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return img_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "images_a, images_b = test_1_1(extract_label, images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing (1 pt)\n",
    "\n",
    "Before computing the Fourier descriptors we need to preprocess the images.\n",
    "\n",
    "* **Q1 (1 pt)**: Complete the function `preprocess` such that it cleans the input images. Take a look at the example images above and try to think what could be improved to allow better uniformity of the data. Take advantage of what you have learned in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply the processing step to images to achieve better data uniformity.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    img_process: np.ndarray (N, 28, 28)\n",
    "        Processed images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shape of input data and set dummy values\n",
    "    n, d, _ = np.shape(images) \n",
    "    img_process = np.zeros_like(images)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return img_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "images_p_a, images_p_b = test_1_2(preprocess, images_a, images_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2 - Fourier Descriptors [17 pts]\n",
    "\n",
    "\n",
    "## 2.1. Get contour and descriptors (9 pts)\n",
    "\n",
    "In this section, we will create Fourier descriptors from digits. The next step in our pipeline will be to detect the contours. To do so you can use existing algorithm available online such as `find_contours`([doc](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.find_contours)) from skcit-image or `findContours` ([doc](https://docs.opencv.org/4.x/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0)) from opencv. Pay attention to the coordinate system when using the contour detection function (is the first component x or y coordinate?).\n",
    "\n",
    "* **Q1 (2 pts)**: Complete the function `find_contour` below such that it returns the contour estimations of the given images. The provided `display_samples` function will display the returned contours for a subset of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contour(images: np.ndarray):\n",
    "    \"\"\"\n",
    "    Find the contours for the set of images\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images to process\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of images to process\n",
    "    N, _, _ = np.shape(images)\n",
    "    # Fill in dummy values (fake points)\n",
    "    contours = [np.array([[0, 0], [1, 1]]) for i in range(N)]\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return contours\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "cnt_p_a, cnt_p_b = test_2_1(find_contour, images_p_a, images_p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we are able to properly detect shape contours, we can finally compute Fourier descriptors. However, we still face a small issue. The allow a fair comparison between the Fourier descriptors we need to ensure that all descriptors have the same length.\n",
    "\n",
    "* **Q2 (2 pts)**: Implement the function `compute_descriptor_padding` that takes as input the computed contours from before and returns the Fourier descriptors for each contour. Use `fft` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html#numpy.fft.fft)) from Numpy to compute the transformation. Use the `n_sample` argument to set the number of points to consider per contour. If the contour is longer than `n_sample` discard the extra points. If the contour is shorter than `n_sample`, use 0 paddings. Make sure that the first element of the descriptor represent the continuous component in the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptor_padding(contours: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Compute Fourier descriptors of input images\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider. If the contour length is higher, discard the remaining part. If it is shorter, add padding.\n",
    "        Make sure that the first element of the descriptor represents the continuous component.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    descriptors: np.ndarray complex (N, n_samples)\n",
    "        Computed complex Fourier descriptors for the given input images\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    # Look for the number of contours\n",
    "    descriptors = np.zeros((N, n_samples), dtype=np.complex128)\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the `plot_features` function to display the computed Fourier descriptors. We display the real, imaginary, and absolute components, respectively. Each color depicts a different digit. We use 2D plots to highlight the clustering capability of the components.\n",
    "* **Q3 (1 pt)**: Comment on the quality of the Fourier descriptors. Do you think they are good feature descriptors? (justify)\n",
    "    * **Answer**: ...\n",
    "* **Q4 (1 pt)**: Knowing that we used `n_samples=11` to compute the Fourier transform, what do the components 0, 1, 5, and 10 represent as frequencies? (high, medium, constant, etc.)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_1_2(compute_descriptor_padding, cnt_p_a, cnt_p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have realized before, the computed Fourier descriptors do not help us to find an optimal separation between the digits. This mainly comes from the fact that using 0 padding is a cheap and inefficient way to ensure homogeneity in length for descriptors. A better solution would be to ensure that we have the same number of points along the contour for each digit. To do so, we need to implement a new function that will resample the contour such that we always have the same number of points no matter the shape.\n",
    "\n",
    "* **Q5 (2 pts)**: Implement the function `linear_interpolation` that takes as input the contours with various lengths and the wanted number of samples per contour for resampling. For each contour, resample the points such that each contour has the same length `n_samples`. We want the points to be uniformly distributed (same distance between points) along the contour. You can use for example the function `interp` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.interp.html)) from Numpy to perform the interpolation. The function `display_samples` will display the contour for different lengths.\n",
    "\n",
    "**Hint**: Think about the contour as two distinct signals x(t) and y(t) that are evaluated at different time steps $t$. Here the time steps can be seen as the Euclidean distance between consecutive samples $t_i$ = $ \\sum_{j=1}^{j \\leq i} \\sqrt{(x_{j} - x_{j-1})^2 + (y_{j} - y_{j-1})^2}$, and $t_0 = 0$. The aim here is to resample the function at new time steps $t_i' = i * \\frac{t_{N-1}}{n_{samples} + 1}$ where $n_{samples}$ is the new number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(contours: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Perform interpolation/resampling of the contour across n_samples.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider along the contour.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    contours_inter: np.ndarray (N, n_samples, 2)\n",
    "        Interpolated contour with n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    contours_inter = np.zeros((N, n_samples, 2))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "        \n",
    "    return contours_inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_1_5(linear_interpolation, cnt_p_a, images_p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our resampling approach we can revisualize the clustering efficiency of our descriptors.\n",
    "\n",
    "* **Q6 (1 pt)**: Comment on the quality of the descriptors. Is it better than before? is there a frequency/component that appears to work better? Does it make sense?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "feat_a, feat_b = test_2_1_6(compute_descriptor_padding, linear_interpolation, cnt_p_a, cnt_p_b, n_samples=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Reconstruction (2 pts)\n",
    "\n",
    "For this part, we will now try to go the other way. Given a Fourier descriptor we will try to retrieve the original shape. \n",
    "* **Q1 (1 pt)** Implement the function `compute_reverse_descriptor` that takes as input a single descriptor and reverses it to x and y coordinates given a number of samples `n_samples`. Use the function `ifft` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.fft.ifft.html)) from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reverse_descriptor(descriptor: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Reverse a Fourier descriptor to xy coordinates given a number of samples.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    descriptor: np.ndarray (D,)\n",
    "        Complex descriptor of length D.\n",
    "    n_samples: int\n",
    "        Number of samples to consider to reverse transformation.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    x: np.ndarray complex (n_samples,)\n",
    "        x coordinates of the contour\n",
    "    y: np.ndarray complex (n_samples,)\n",
    "        y coordinates of the contour\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.zeros(n_samples)\n",
    "    y = np.zeros(n_samples)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function `plot_reconstruction` we plot an example of the reconstruction of the digits. We display the result as we progressively add more frequencies. We start with the first component (component 0) and then add frequency pairs from low to high frequencies.\n",
    "\n",
    "* **Q2 (1 pt)**: Based on your observation, do you think 11 samples are enough to properly describe the digits below? (justify)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_2(images_p_a , images_p_b, feat_a, feat_b, compute_reverse_descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Invariance (6 pts)\n",
    "\n",
    "For the last part with Fourier descriptors, we will check for descriptors invariance. As seen in class, if handled properly Fourier descriptors can be invariant to translation, rotation, and scaling.\n",
    "\n",
    "* **Q1 (3 pts)**: Implement the functions `apply_rotation`, `apply_scaling`, and `apply_translate` to apply random rotation, scaling, and translation to input images. For scaling and translation, we recommend avoiding large values where the digits are cropped (out of frame). You can use the `random` package ([doc](https://numpy.org/doc/1.16/reference/routines.random.html)) from Numpy to generate random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotation(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random rotation to input the image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    rotated: np.ndarray (28, 28)\n",
    "        Rotated source images\n",
    "    \"\"\"\n",
    "\n",
    "    rotated = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "\n",
    "def apply_scaling(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random scaling to input image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    scaled: np.ndarray (28, 28)\n",
    "        Scaled source images\n",
    "    \"\"\"\n",
    "    \n",
    "    scaled = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def apply_translate(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random x and y translation to input image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    translated: np.ndarray (28, 28)\n",
    "        Translated source images\n",
    "    \"\"\"\n",
    "    \n",
    "    translated = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_3(apply_rotation, apply_scaling, apply_translate, images_p_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we implemented our transformation, we can test for feature invariance. To assess the robustness of the Fourier descriptors to image transformations, we compute the error between the original descriptors (before transformation) to the one after transformation. \n",
    "\n",
    "* **Q2 (1 pt)**: Complet the function `translation_invariant` to make the Fourier deciptor invariant to translation. Does the error decrease after treatment for translation invariance? Is it null, if not why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to translation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to translation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "feat_t_a = test_2_3_2(translation_invariant, find_contour, apply_translate, compute_descriptor_padding, linear_interpolation,images_p_a, feat_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Complet the function `rotation_invariant` to make the Fourier deciptor invariant to rotation. Does the error decrease after treatment for rotation invariance? Do you think that selecting specific rotation angles might help the error to decrease even lower?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to rotation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to rotation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_3_3(rotation_invariant, find_contour, apply_rotation, compute_descriptor_padding, linear_interpolation,images_p_a, feat_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q4 (1 pt)**: Complet the function `scaling_invariant` to make the Fourier deciptor invariant to scaling. Does the error decrease after treatment for scaling invariance? Is it null? Why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to scaling.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to scaling of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_2_3_4(scaling_invariant, find_contour, apply_scaling, compute_descriptor_padding, linear_interpolation,images_p_a, feat_t_a, feat_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3 - Other descriptors [8 pts]\n",
    "\n",
    "## 3.1 Distance map (5 pts)\n",
    "\n",
    "In this part, we will learn how to use a distance map as a feature descriptor. \n",
    "\n",
    "* **Q1 (1 pt)**: To compute a distance map we first need a reference pattern. Complete the function `reference_pattern`. The function takes as input a list of images and computes the reference pattern as the average of all shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_pattern(imgs):\n",
    "    \"\"\"\n",
    "    Compute the reference pattern for a given set of images. The reference pattern \n",
    "    is estimated as the average of all images of the same pattern.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    pattern: np.ndarray (28, 28)\n",
    "        Thresholded reference pattern that is the average of all shapes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize pattern\n",
    "    pattern = np.zeros((imgs[0].shape[0], imgs[0].shape[1]))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "   \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "pattern_a, pattern_b = test_3_1(reference_pattern, images_p_a, images_p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q2 (2 pts)**: The next part will be to compute a distance map from the generated pattern. By pre-computing the distance map we can speedup the inference time. Complete the function `compute_distance_map`. We expect the values of the map to represent the distances to the closest pattern contour. If needed, can take advantage of the functions you wrote in PART2 to detect and resample contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_map(pattern: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the distance map for the given pattern. The values of the map are computed as \n",
    "    the distance to the closest pattern contour.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    pattern: np.ndarray (28, 28)\n",
    "        Pattern to process\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    distance_map: np.ndarray (28, 28)\n",
    "        Distance map where each entry is the distance to the closest pattern contour (shortest \n",
    "        distance to pattern)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dummy values\n",
    "    distance_map = np.zeros_like(pattern)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return distance_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "map_a, map_b = test_3_1_2(compute_distance_map, pattern_a, pattern_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q3 (2 pts)**: For the last part, implement the function `compute_distance` that uses the precomputed distance map to evaluate the distance to all images. Note that for each image you should return the average of distances. As before, for each digit, you can compute the contour and estimate the point-to-point distance by evaluating the distance map at the xy contour coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(imgs, d_map):\n",
    "    \"\"\"\n",
    "    Compute the distances for each image with respect to the reference pattern using the precomputed \n",
    "    distance map. The final distance is the average of all distances from the image's contour points \n",
    "    to the reference pattern.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "    d_map: np.ndarray (28, 28)\n",
    "        The precomputed distance map where each entry is the distance to the closest pattern contour \n",
    "        (shortest distance to pattern)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dist: np.ndarray (N, )\n",
    "        Averaged distance to pattern for each input image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default values\n",
    "    dist = np.zeros(len(imgs))\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_3_1_3(compute_distance, images_p_a, images_p_b, map_a, map_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Others (3 pts)\n",
    "\n",
    "For the last part of the lab, we will compute other various visual features. \n",
    "\n",
    "* **Q1: (2 pts)**: Implement the function `compute_features` that estimates the digit's perimeter, area, compacity, and rectangularity. You can use the lecture to look for the definition of each feature. To help you with this task we strongly recommend using the `regionprops` ([doc](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops)) from scikit-image.\n",
    "* **Q2: (1 pt)**: Which feature(s) seem(s) to show the best results to distinguish between both digits? (justify)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(imgs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute compacity for each input image.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    f_peri: np.ndarray (N,)\n",
    "        Estimated perimeter length for each image\n",
    "    f_area: np.ndarray (N,)\n",
    "        Estimated area for each image\n",
    "    f_comp: np.ndarray (N,)\n",
    "        Estimated compacity for each image\n",
    "    f_rect: np.ndarray (N,)\n",
    "        Estimated rectangularity for each image\n",
    "    \"\"\"\n",
    "\n",
    "    f_peri = np.zeros(len(imgs))\n",
    "    f_area = np.zeros(len(imgs))\n",
    "    f_comp = np.zeros(len(imgs))\n",
    "    f_rect = np.zeros(len(imgs))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return f_peri, f_area, f_comp, f_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############################ TEST ##############################\n",
    "################################################################\n",
    "\n",
    "test_3_2(compute_features, images_p_a, images_p_b)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
