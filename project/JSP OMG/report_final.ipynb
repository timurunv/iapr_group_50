{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0831a98d",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** 50  \n",
    "**Kaggle challenge:** Classic  \n",
    "**Kaggle team name (exact):** \"ChocoBlabla\"  \n",
    "\n",
    "**Author 1 (SCIPER):** *Louis Cuendet (xxxxxx)*  \n",
    "**Author 2 (SCIPER):** *Timur Ünver (xxxxx)*  \n",
    "**Author 3 (SCIPER):** *Adrien Boschung (327221)*  \n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81a7fc",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e32fb9",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c31ef",
   "metadata": {},
   "source": [
    "In this section, we will detail which chocolate descriptors we decide to keep or not for classification and the reasons why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40cdb8",
   "metadata": {},
   "source": [
    "The first important note is that the descriptors were computed on the reference images. We did not take into account other chocolates from the train dataset as we developed segmentation and classification in parallel and it was therefore easier to develop classification regarding the reference images that were much easier to segment. An improvement to our classification would be to average the chocolate descriptors accross the train set which would most probably yield a better result on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcb874",
   "metadata": {},
   "source": [
    "Our first idea was to first clusterize the chocolates in different categorize related to their shapes. To do so we decided to compute the Fourier descriptors as they worked well during the labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87318ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contours\n",
    "contours_patterns = []\n",
    "\n",
    "def contour_of_patterns(patterns):\n",
    "    contours_patterns = []\n",
    "    for i in range(len(patterns)):\n",
    "        if i < n_images:\n",
    "            plt.figure()\n",
    "            img = patterns[i]\n",
    "            img = np.array(img)\n",
    "            img = np.mean(img, axis=2)\n",
    "            binary = img > 0 \n",
    "            contours = find_contours(binary, level=0.5)\n",
    "            if contours:\n",
    "                contour = np.fliplr(max(contours, key=lambda x: x.shape[0]))\n",
    "                contours_patterns.append(contour)\n",
    "                plt.imshow(patterns[i], cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.plot(contour[:, 0], contour[:, 1], linewidth=2, color='red')  # use [:, 0] and [:, 1]\n",
    "        else:\n",
    "            ax.axis('off')  # Hide empty subplots\n",
    "\n",
    "    return contours_patterns\n",
    "\n",
    "contours_patterns = contour_of_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32aa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptor_padding(contours: np.ndarray, n_samples: int = 9):\n",
    "    \"\"\"\n",
    "    Compute Fourier descriptors of input images\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider. If the contour length is higher, discard the remaining part. If it is shorter, add padding.\n",
    "        Make sure that the first element of the descriptor represents the continuous component.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    descriptors: np.ndarray complex (N, n_samples)\n",
    "        Computed complex Fourier descriptors for the given input images\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    # Look for the number of contours\n",
    "    descriptors = np.zeros((N, n_samples), dtype=np.complex128)\n",
    "\n",
    "    # ------------------\n",
    "    for i in range(N):\n",
    "        contour = contours[i]\n",
    "        if len(contour) > n_samples:\n",
    "            contour = contour[:n_samples]\n",
    "        elif len(contour) < n_samples:\n",
    "            contour = np.concatenate((contour, np.zeros((n_samples - len(contour), 2))), axis=0)\n",
    "        descriptors[i, :] = np.fft.fft(contour[:, 0] + 1j * contour[:, 1], n_samples, axis=0)\n",
    "    # ------------------\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "def linear_interpolation(contours: np.ndarray, n_samples: int = 9):\n",
    "    \"\"\"\n",
    "    Perform interpolation/resampling of the contour across n_samples.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider along the contour.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    contours_inter: np.ndarray (N, n_samples, 2)\n",
    "        Interpolated contour with n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    contours_inter = np.zeros((N, n_samples, 2))\n",
    "    \n",
    "    # ------------------\n",
    "    for i in range(N):\n",
    "        # Get the contour to process\n",
    "        contour = contours[i]\n",
    "        # Compute the distance between points\n",
    "        dist = np.sqrt(np.sum(np.diff(contour, axis=0) ** 2, axis=1))\n",
    "        # Compute the cumulative distance\n",
    "        cum_dist = np.concatenate(([0], np.cumsum(dist))) #with initial point at 0\n",
    "        # Interpolate the points\n",
    "        dist = np.linspace(0, cum_dist[-1], n_samples) #defining the distance axis (from 0 to the total distance at [-1] index)\n",
    "        x_interp = np.interp(dist, cum_dist, contour[:, 0]) #x as a function of distance\n",
    "        y_interp = np.interp(dist, cum_dist, contour[:, 1]) #y as a function of distance\n",
    "        contours_inter[i] = np.array([x_interp, \n",
    "                                      y_interp]).T #transposing to get the right shape\n",
    "    # ------------------\n",
    "        \n",
    "    return contours_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to translation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to translation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    features_inv[:,1:] = features[:,1:]\n",
    "    # ------------------\n",
    "    \n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to rotation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to rotation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    features_inv = np.abs(features)\n",
    "    # ------------------\n",
    "    \n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887560da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_descriptors(descriptors):\n",
    "    normalized = []\n",
    "    for des in descriptors:\n",
    "        # Remove the DC component (translation invariance)\n",
    "        des[0] = 0\n",
    "        # Scale invariance\n",
    "        des = des / np.abs(des[1])\n",
    "        # Use only magnitudes (rotation invariance)\n",
    "        normalized.append(np.abs(des))\n",
    "    return np.array(normalized)\n",
    "\n",
    "normalized_descriptors = normalize_descriptors(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e991b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the different shapes we average the descriptors\n",
    "correct_labels = [0,1,1,1,0,0,1,2,3,3,2]\n",
    "print(correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = np.array(correct_labels)\n",
    "unique_classes = np.unique(correct_labels)\n",
    "mean_descriptors = {}\n",
    "\n",
    "# Compute the mean descriptors for each class\n",
    "for class_label in unique_classes:\n",
    "    indices = np.where(correct_labels == class_label)[0]\n",
    "    mean_descriptors[class_label] = np.mean(normalized_descriptors[indices], axis=0)\n",
    "\n",
    "# Print the mean descriptors for each class\n",
    "for class_label, mean_descriptor in mean_descriptors.items():\n",
    "    print(f\"Class {class_label}: Mean Descriptor\")\n",
    "    print(mean_descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d2fce",
   "metadata": {},
   "source": [
    "Now we will plot the mean Fourier descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for d in mean_descriptors:\n",
    "    plt.plot(mean_descriptors[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc5775",
   "metadata": {},
   "source": [
    "We see that the descriptors are very similar are therefore very hard to differentiate. They worked better in the labs as the numbers we were trying to retrieve were very different but in our case, the shape are similar and irregularities/inconsistencies in segmentation can easily lead in differences in the Fourier descriptor that will prevent us from correctly classifying the chocolate. For these reasons we decided to drop Fourier descriptors and focus on other features that are more insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33929d69",
   "metadata": {},
   "source": [
    "To further show you the similarities between the descriptors, we computed the how similar shapes are relative to each other (with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def to_cv2_contour(contour):\n",
    "    return contour.reshape((-1, 1, 2)).astype(np.float32)\n",
    "\n",
    "# Convert all contours to OpenCV format\n",
    "cv2_contours = [to_cv2_contour(c) for c in contours_patterns]\n",
    "\n",
    "# Create similarity matrix\n",
    "n = len(cv2_contours)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        similarity_matrix[i, j] = cv2.matchShapes(cv2_contours[i], cv2_contours[j], cv2.CONTOURS_MATCH_I1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Optional: chocolate names for x/y labels\n",
    "choco_names = [\n",
    "    \"Amandina\", \"Arabia\", \"Comtesse\", \"Crème Brulée\", \"Jelly Black\", \"Jelly Milk\", \"Jelly White\",\n",
    "     \"Noblesse\", \"Noir authentique\", \"Passion au lait\", \"Stracciatella\", \"Tentation noir\", \"Triangolo\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(scaled, xticklabels=choco_names, yticklabels=choco_names, cmap='viridis', annot=True, fmt=\".3f\")\n",
    "plt.title(\"Shape Similarity Matrix (Lower = More Similar)\")\n",
    "plt.xlabel(\"Chocolate Class\")\n",
    "plt.ylabel(\"Chocolate Class\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c2e07",
   "metadata": {},
   "source": [
    "We see that chocolates are close to each other except for amandina and triangolo. For amandina there is another feature that we will develop later which already strongly differentiate amandina from the rest. In general, we decided to not use the Fourier descriptors due to imperfections in segmention which negatively impacts the Fourier descriptors and their use for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac6d05",
   "metadata": {},
   "source": [
    "Even though shapes are not useful by themselves, other geometric features are. By investigating them we found a few that successfully cluster the chocolates in three different categories. The 1st one is the compacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a61f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(contours_patterns) :\n",
    "    areas = []\n",
    "    perimeters = []\n",
    "    ratio = []\n",
    "    compacity = []\n",
    "    \n",
    "    for contour in contours_patterns:\n",
    "        contour = contour.astype(np.float32)\n",
    "        areas.append(cv2.contourArea(contour))\n",
    "        perimeters.append(cv2.arcLength(contour, closed=True))\n",
    "        ratio.append(cv2.contourArea(contour)/cv2.arcLength(contour, closed=True))\n",
    "        compacity.append(cv2.contourArea(contour)**2/cv2.arcLength(contour, closed=True))\n",
    "        #print(f'Area = {area}, Perimeter = {perimeter}')\n",
    "\n",
    "    \"\"\" plt.figure()\n",
    "    plt.plot(range(len(perimeters)), perimeters, '.')\n",
    "    plt.axhline(y=1750, color='green', linestyle='--')\n",
    "    plt.axhline(y=1500, color='green', linestyle='--')\n",
    "    plt.axhline(y=2200, color='green', linestyle='--') \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    x_pos = [1] * len(perimeters)  # All x-values are 1\n",
    "    plt.plot(x_pos, perimeters, 'o')  # Use 'o' for point markers\n",
    "    #plt.plot(1750, color='green')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Perimeter')\n",
    "    plt.title('Perimeters on a single vertical line')\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\" plt.figure()\n",
    "    plt.plot(range(len(areas)), areas, '.') \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    x_pos = [1] * len(areas)  # All x-values are 1\n",
    "    plt.plot(x_pos, areas, 'o')  # Use 'o' for point markers\n",
    "    #plt.plot(1750, color='green')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Area')\n",
    "    plt.title('Area on a single vertical line')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    x_pos = [1] * len(ratio)  # All x-values are 1\n",
    "    plt.plot(x_pos, ratio, 'o')  # Use 'o' for point markers\n",
    "    #plt.axhline(y=102, color='red', linestyle='--', label='Threshold 1')\n",
    "    #plt.axhline(y=121, color='green', linestyle='--', label='Threshold 2')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Ratio on a single vertical line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    x_pos = [1] * len(compacity)  # All x-values are 1\n",
    "    plt.plot(x_pos, compacity, 'o')  # Use 'o' for point markers\n",
    "    #plt.axhline(y=100, color='red', linestyle='--', label='Threshold 1')\n",
    "    #plt.axhline(y=120, color='green', linestyle='--', label='Threshold 2')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Compacity')\n",
    "    plt.title('Compacity on a single vertical line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\" \n",
    "    plt.figure()\n",
    "    plt.plot(areas, perimeters, '.') \"\"\"\n",
    "    return areas, perimeters, ratio, compacity\n",
    "\n",
    "areas, perimeters, ratio, compacity = find_clusters(contours_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a5570",
   "metadata": {},
   "source": [
    "We see on the compacity plot (and area/perimeter) that we can extract two clusters. Chocolates with a compacity higher and lower than approximately 320'000. The 3 low compacity chocolates are the 3 Jelly's (Cluster 2). This is a good cluster as we can easily differentiate the Jelly chocolates by their RGB colors which we will show later.\n",
    "Now we want to investigate if we can find another feature to further separate the bigger cluster of 10 chocolates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a8a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ratio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cluster1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m cluster2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mratio\u001b[49m)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compacity[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m320000\u001b[39m :\n\u001b[0;32m      5\u001b[0m         cluster1\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ratio' is not defined"
     ]
    }
   ],
   "source": [
    "cluster1 = []\n",
    "cluster2 = []\n",
    "for i in range(len(ratio)):\n",
    "    if compacity[i] > 320000 :\n",
    "        cluster1.append(i)\n",
    "        continue\n",
    "    else :\n",
    "        cluster2.append(i)\n",
    "\n",
    "print(cluster1)\n",
    "print(cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas1, perimeters1, ratio1, compacity1 = find_clusters([contours_patterns[i] for i in cluster1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c10e62",
   "metadata": {},
   "source": [
    "On these graphs specific to the first cluster we see another feature that can clusterize the chocolates which is the ratio of the area over the perimeter. It separates high ratio chocolates (Cluster 1.1 : Crème Brulée, Noir authentique and Passion au lait) from the low ratio chocolates (CLuster 1.2 : Amandina, Arabia, Comtesse, Noblesse, Straciatella and Triangolo). These cluster seem good as each chocolates is quite different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58f5c2",
   "metadata": {},
   "source": [
    "We will of course color features (RGB) as well as HSV because they are basic features that have different values for different chocolates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416ef35",
   "metadata": {},
   "source": [
    "For each cluster, we build a different KNN classifier with the features extracted from the reference images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fbec0",
   "metadata": {},
   "source": [
    "#### CLuster 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7736f",
   "metadata": {},
   "source": [
    "This cluster contains Crème Brulée, Noir authentique and Passion au lait."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499ab40",
   "metadata": {},
   "source": [
    "For this cluster we decided to use the following features: \n",
    "- RGB \n",
    "- HSV \n",
    "- Contrast\n",
    "- Stripe detection\n",
    "- Perimeter\n",
    "- Area\n",
    "- Circularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8ae06",
   "metadata": {},
   "source": [
    "For this cluster, we observed surprising behavior. The performance on the train set was the highest when the only features used to differentiate them was the RGB values (no HSV nor contrast nor stripe detection) and then we tried to add perimeter, area and circularity which slightly improved performance. This is surprising, especially for HSV as the 3 chocolates have very different HSV values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f3ac",
   "metadata": {},
   "source": [
    "When fine tuning the classification, we observed that sometimes Triangolo's would be clusterize in 1.1 instead of 1.2. When adding Triangolo to 1.1 (and keeping it in 1.2 at the same time) we obtained better results. We do not think it is an issue to have a chocolate in both clusters as the other descriptors are different and will not lead to major misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba4180",
   "metadata": {},
   "source": [
    "#### CLuster 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8a566",
   "metadata": {},
   "source": [
    "This cluster contains Amandina, Arabia, Comtesse, Noblesse, Straciatella, Tentation noir and Triangolo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf575350",
   "metadata": {},
   "source": [
    "For this cluster we decided to use the following features: \n",
    "- RGB \n",
    "- HSV \n",
    "- Texture\n",
    "- Rectangularity\n",
    "- Contrast\n",
    "- Stripe detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e27da",
   "metadata": {},
   "source": [
    "Stripe detection was used to classify Straciatella's as it contains white stripes. When classifying it in the KNN it led to some misclassification with Tentation noir and Arabia (but they have no stripes). This improved our performance on the train set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80bde0e",
   "metadata": {},
   "source": [
    "The rectangularity feature was used to try to differentiate the triangolo and tentation noir from the circular chocolates. We wanted looked into ways of quantifying triangularity such as peaks detection but it was hard to extract specific values for triangolo out of them (segmentation and other issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284b19c",
   "metadata": {},
   "source": [
    "Contrast results in a wide range of values for the chocolates and was therefore deemed as a performant feature for the KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598ac8e",
   "metadata": {},
   "source": [
    "#### CLuster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a64166",
   "metadata": {},
   "source": [
    "This cluster contains Jelly White, Jelly Milk and Jelly Black."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506072b1",
   "metadata": {},
   "source": [
    "For this cluster we decided to use the following features: \n",
    "- RGB \n",
    "- HSV \n",
    "- Contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab74f7a",
   "metadata": {},
   "source": [
    "Jelly's all have the same shape so geometrical features will not be relevant in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846562d",
   "metadata": {},
   "source": [
    "On the other hand they all have distinct colors so RGB and HSV are almost sufficient to classify them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f7d1b",
   "metadata": {},
   "source": [
    "We also added contrast ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0a16c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5f7f55",
   "metadata": {},
   "source": [
    "#### Implementations for special cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd846ca",
   "metadata": {},
   "source": [
    "Black boxes and magnets : \n",
    "\n",
    "In the images there are some objects (black items and magnets) that have a similar size and shape as the chocolates that we want to identify. To try to recognize them and ignore them we sampled their RGB values in the train set and if the the object we are trying to classify has an RGB too close to these, we ignore it. This improved our performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d8b31",
   "metadata": {},
   "source": [
    "Smoothing chocolates :\n",
    "\n",
    "It oftens happens that part of the shadow is included in the cropped out chocolate. In this case smoothing (more specifically opening) the cropped out chocolate can help to erase this unwanted feature and help correctly classify the chocolates. It also improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f139dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply opening to smoothen the contour\n",
    "footprint = disk(15)\n",
    "smoothed_mask = opening(mask_crop, footprint=footprint)\n",
    "isolated_img = np.zeros_like(img_crop)\n",
    "for c in range(3):\n",
    "    isolated_img[..., c] = img_crop[..., c] * smoothed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f453d05",
   "metadata": {},
   "source": [
    "Watershed :\n",
    "\n",
    "This algorithm is used to separate chocolates that are glued together. It can sometime fail to separate the chocolates in specific cases. For examples if two Triangolo's are standing next to each other side to side and not peak to peak it will fail to separate them and identify them successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88278543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large object: apply Watershed to split it\n",
    "region_crop = isolated_mask[minr:maxr, minc:maxc].astype(np.uint8)\n",
    "chocolate_crop = segmented_image[minr:maxr, minc:maxc]\n",
    "\n",
    "# Create markers using distance transform\n",
    "dist = cv2.distanceTransform(region_crop * 255, cv2.DIST_L2, 5)\n",
    "_, sure_fg = cv2.threshold(dist, 0.9 * dist.max(), 255, 0)\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(region_crop * 255, sure_fg)\n",
    "\n",
    "# Marker labelling\n",
    "_, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = markers + 1\n",
    "markers[unknown == 255] = 0\n",
    "\n",
    "# Watershed\n",
    "chocolate_crop_color = chocolate_crop.copy()\n",
    "markers = cv2.watershed(chocolate_crop_color, markers)\n",
    "\n",
    "# Loop over each separated region (label > 1)\n",
    "isolated_imgs = []\n",
    "for label_val in np.unique(markers):\n",
    "    if label_val <= 1:\n",
    "        continue  # Skip background and unknown\n",
    "\n",
    "    mask_ws = (markers == label_val).astype(np.uint8)\n",
    "    if np.sum(mask_ws) < 10000:\n",
    "        continue  # Skip tiny blobs\n",
    "\n",
    "    # Create masked chocolate\n",
    "    masked_choco = np.zeros_like(chocolate_crop_color)\n",
    "    for c in range(3):\n",
    "        masked_choco[..., c] = chocolate_crop_color[..., c] * mask_ws\n",
    "\n",
    "    isolated_imgs.append(masked_choco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab10f1",
   "metadata": {},
   "source": [
    "Various Thresholds :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093e214",
   "metadata": {},
   "source": [
    "It happens that objects segmented cannot be chocolates if they are too large or too small. In those cases we have thresholds to ignore too large and too small objects. There is a special thresholding for objects that potentially correspond to 2 chocolates sticked together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdeb78",
   "metadata": {},
   "source": [
    "#### Abandoned ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96366422",
   "metadata": {},
   "source": [
    "Weights : \n",
    "\n",
    "An idea we had was to weight the different features depending on how much they contribute to the chocolate classification. To our surprise the performance on the train set was better when not weighting the features so we abandoned this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef866a",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30bc58",
   "metadata": {},
   "source": [
    "Mahalanobis distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
